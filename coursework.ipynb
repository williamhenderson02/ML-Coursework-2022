{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# - Notes for use to avoid confusion :)\n",
    "\n",
    "\n",
    "#Not all cells work together. For example if you run the cell that deals with missing values by removing the entire row\n",
    "#or removing all columns you will not be able to use the method to predict null values because the entries will be gone.\n",
    "\n",
    "#So in order to predict the missing values after using the code cells that remove them you simply have to first run the code cell\n",
    "#which reads the original data again\n",
    "\n",
    "#To do:\n",
    "    #Look into outliers\n",
    "    #3D graphs\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values have value ? all for attribute 11\n",
    "\n",
    "#either edible, poisonous or unknown. Unknowm and poisonous grouped together\n",
    "\n",
    "#to deal with missing values use 'classifier' or 'regressor' based on values that arent missing to predict missing values in\n",
    "#preprocessing. Be careful that vlaues using to predict arent overfitted. Compare to other methods such as removing entire attribute\n",
    "#or giving deafult/mean value\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import ensemble\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import umap.umap_ as umap\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read contents of data file\n",
    "file = pd.read_csv(\"agaricus-lepiota.data\", header = None)\n",
    "\n",
    "# create list to be used as headers\n",
    "features = ['edibility', 'capShape', 'capSurface', 'capColor', 'bruises', 'odor', \n",
    "            'gillAttachment', 'gillSpacing', 'gillSize', 'gillColor', \n",
    "            'stalkShape', 'stalkRoot', 'stalkSurfaceAboveRing', 'stalkSurfaceBelowRing', \n",
    "            'stalkColorAboveRing', 'stalkColorBelowRing', 'veilType', 'veilColor', \n",
    "            'ringNumber', 'ringType', 'sporePrintColor', 'population', 'habitat']\n",
    "\n",
    "#convert '?' to NaN\n",
    "file.replace({'?': np.nan}, inplace=True)\n",
    "\n",
    "# converting data frame to csv\n",
    "file.to_csv(\"agaricus-lepiota.csv\", header=features, index=False)\n",
    "\n",
    "data = pd.read_csv(\"agaricus-lepiota.csv\")\n",
    "\n",
    "#we can drop veilType as all entries are the same so cannot be used to predict\n",
    "data.nunique()\n",
    "data = data.drop('veilType', axis = 1).reset_index(drop=True)\n",
    "\n",
    "#result = data.head(10)\n",
    "#print(data)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes column with missing values which in our data is only stalkRoot\n",
    "#one way to deal with missing values\n",
    "print(data.shape)\n",
    "\n",
    "data = data.drop('stalkRoot', axis = 1).reset_index(drop=True)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data.to_csv(\"agaricus-lepiota-no-stalkRoot.csv\")\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes all rows that have missing values\n",
    "print(data.shape)\n",
    "\n",
    "data = data.dropna(axis = 0).reset_index(drop=True)\n",
    " \n",
    "#shows we lose alot of data as 2480 rows lost\n",
    "#bad way of handling missing values\n",
    "print(data.shape)\n",
    "\n",
    "data.to_csv(\"agaricus-lepiota-NaN-removed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use label encoding on all featres that are categorical\n",
    "#OneHot encoding not applicable as there are a high number of categories\n",
    "le = LabelEncoder()\n",
    "\n",
    "data['edibility'] = le.fit_transform(data['edibility'])\n",
    "data['capShape'] = le.fit_transform(data['capShape'])\n",
    "data['capSurface'] = le.fit_transform(data['capSurface'])\n",
    "data['capColor'] = le.fit_transform(data['capColor'])\n",
    "data['bruises'] = le.fit_transform(data['bruises'])\n",
    "data['odor'] = le.fit_transform(data['odor'])\n",
    "data['gillAttachment'] = le.fit_transform(data['gillAttachment'])\n",
    "data['gillSpacing'] = le.fit_transform(data['gillSpacing'])\n",
    "data['gillSize'] = le.fit_transform(data['gillSize'])\n",
    "data['gillColor'] = le.fit_transform(data['gillColor'])\n",
    "data['stalkShape'] = le.fit_transform(data['stalkShape'])\n",
    "data['stalkRoot'] = le.fit_transform(data['stalkRoot'])\n",
    "data['stalkSurfaceAboveRing'] = le.fit_transform(data['stalkSurfaceAboveRing'])\n",
    "data['stalkSurfaceBelowRing'] = le.fit_transform(data['stalkSurfaceBelowRing'])\n",
    "data['stalkColorAboveRing'] = le.fit_transform(data['stalkColorAboveRing'])\n",
    "data['stalkColorBelowRing'] = le.fit_transform(data['stalkColorBelowRing'])\n",
    "data['veilColor'] = le.fit_transform(data['veilColor'])\n",
    "data['ringNumber'] = le.fit_transform(data['ringNumber'])\n",
    "data['ringType'] = le.fit_transform(data['ringType'])\n",
    "data['sporePrintColor'] = le.fit_transform(data['sporePrintColor'])\n",
    "data['population'] = le.fit_transform(data['population'])\n",
    "data['habitat'] = le.fit_transform(data['habitat'])\n",
    "\n",
    "data.to_csv(\"agaricus-lepiota-encoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       724\n",
      "           1       1.00      1.00      1.00       119\n",
      "           2       1.00      1.00      1.00       239\n",
      "           3       1.00      1.00      1.00        47\n",
      "\n",
      "    accuracy                           1.00      1129\n",
      "   macro avg       1.00      1.00      1.00      1129\n",
      "weighted avg       1.00      1.00      1.00      1129\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       724\n",
      "           1       1.00      1.00      1.00       119\n",
      "           2       1.00      1.00      1.00       239\n",
      "           3       1.00      1.00      1.00        47\n",
      "\n",
      "    accuracy                           1.00      1129\n",
      "   macro avg       1.00      1.00      1.00      1129\n",
      "weighted avg       1.00      1.00      1.00      1129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predict missing values for stalkRoot\n",
    "NaN_predict = data.drop('edibility', axis = 1).reset_index(drop=True)\n",
    "\n",
    "NaN_train_data = NaN_predict[NaN_predict['stalkRoot']!= 4].copy()\n",
    "NaN_test_data = NaN_predict[NaN_predict['stalkRoot'] == 4].copy()\n",
    "\n",
    "NaN_X_train = NaN_train_data.drop('stalkRoot', axis = 1).reset_index(drop=True)\n",
    "NaN_y_train = NaN_train_data['stalkRoot']\n",
    "\n",
    "NaN_X_test = NaN_test_data.drop('stalkRoot', axis = 1).reset_index(drop=True)\n",
    "\n",
    "###############################################################################################################################\n",
    "#This code further breaks the data down into training and test sets comprising of rows that have no missing values\n",
    "#Allows us to run models and check how accurately they can predict the missing values then choose an appropriate one\n",
    "#In this case both Random Forest and Support Vector Classification predict that exact same values for 'stalkRoot' so either can be used\n",
    "#Both have 100% accuracy in predicitng 'stalkRoot' so we know when we predict the missing values they will most likely be correct.\n",
    "X_train, X_test, y_train, y_test = train_test_split(NaN_train_data, NaN_train_data['stalkRoot'], test_size=0.2,random_state=42)\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "svm = SVC()\n",
    "\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_pred = rfr.predict(X_test)\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "svm_pred = svm.predict(X_test)\n",
    "\n",
    "rfr_rounded = (np.rint(rfr_pred)).astype(int)\n",
    "\n",
    "svm_rounded = (np.rint(svm_pred)).astype(int)\n",
    "\n",
    "different = []\n",
    "\n",
    "for i in range(len(rfr_rounded)):\n",
    "    if rfr_rounded[i] != svm_rounded[i]:\n",
    "        different.append(rfr_rounded[i], svm_rounded[i])\n",
    "\n",
    "print(len(different))\n",
    "\n",
    "print(classification_report(y_test,rfr_rounded))\n",
    "\n",
    "print(classification_report(y_test,svm_rounded))\n",
    "##############################################################################################################################\n",
    "\n",
    "rfc = RandomForestRegressor()\n",
    "\n",
    "rfc.fit(NaN_X_train,NaN_y_train)\n",
    "y_pred = rfc.predict(NaN_X_test)\n",
    "\n",
    "rounded_data = (np.rint(y_pred)).astype(int)\n",
    "NaN_test_data['stalkRoot'] = rounded_data\n",
    "\n",
    "NaN_test_data.to_csv(\"predicted-missing-values.csv\")\n",
    "\n",
    "frames = [NaN_train_data, NaN_test_data]\n",
    "treated_data = pd.concat(frames)\n",
    "treated_data.insert(0, column = 'edibility', value = data['edibility'])\n",
    "\n",
    "X = treated_data.drop('edibility', axis = 1).reset_index(drop=True)\n",
    "y = treated_data['edibility']\n",
    "\n",
    "data_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "treated_data.to_csv(\"full-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variance inflation factor\n",
    "\n",
    "fig = plt.figure(figsize=(25, 20))\n",
    "sns.heatmap(X.corr(), annot=True)\n",
    "plt.savefig(\"Heatmap.png\")\n",
    "\n",
    "#as you can see from heatmap there are some fearutes with high correlation\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X.columns\n",
    "\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
    "\t\t\t\t\t\tfor i in range(len(X.columns))]\n",
    "\n",
    "print(vif_data)\n",
    "\n",
    "features_to_drop = []\n",
    "\n",
    "for i in range(len(X.columns)):\n",
    "\tif vif_data[\"VIF\"][i] > 7:\n",
    "\t\tdrop = vif_data[\"VIF\"][i]\n",
    "\t\tfeatures_to_drop.append(vif_data[\"feature\"][i])\n",
    "\t\n",
    "new_X = X.drop(features_to_drop, axis = 1).reset_index(drop=True)\n",
    "\n",
    "#generally features with VIF > 5 get removed but we can experiment with slightly higher i.e > 7\n",
    "\n",
    "fig2 = plt.figure(figsize=(25, 20))\n",
    "sns.heatmap(new_X.corr(), annot=True)\n",
    "plt.savefig(\"Heatmap2.png\")\n",
    "\n",
    "#as seen by second heatmap these have much less colinearality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data visualisation pca\n",
    "#a linear method so less complex than tsne thus takes much less time\n",
    "#not very useful to visualise our dataset I dont think as shown by the explained variance and scree plot\n",
    "pca = PCA()\n",
    "pca_features = pca.fit_transform(data_scaled)\n",
    "\n",
    "per_var = np.round(pca.explained_variance_ratio_*100, decimals = 1)\n",
    "print(per_var)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    "\n",
    "#to be seen on a 2D-plot we can only use PC1 and PC2 which gives us the highest explained variance but still very low\n",
    "#around 19% + 12% = 31% explained variance which is bad\n",
    "plt.bar(x= range(1,len(per_var)+1), height = per_var, tick_label = labels)\n",
    "plt.show()\n",
    "\n",
    "pca_df = pd.DataFrame({'pca_1': pca_features[:,0], 'pca_2': pca_features[:,1], 'label': treated_data['edibility']})\n",
    "print(pca_df)\n",
    "\n",
    "sns.scatterplot(x = 'pca_1', y = 'pca_2', hue = 'label', data = pca_df)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#PCA could be useful to reduce multicolinearality though instead of data visualisation\n",
    "#use the first 8 pc's to get about ~90% explained variance, 9 to get ~93%, 10 to get ~95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data visualisation t-sne\n",
    "#unlike pca its not linear so much more complex and better for our data I think\n",
    "#result of t-sne greatly affected by 'perplexity' hyperparameter\n",
    "#try perplexity values in the range 5-100 default is 30. Higher perplexity takes longer but may be better\n",
    "n_components = 2\n",
    "tsne = TSNE(n_components, perplexity=5)\n",
    "tsne_features = tsne.fit_transform(data_scaled)\n",
    "\n",
    "tsne_df = pd.DataFrame({'tsne_1': tsne_features[:,0], 'tsne_2': tsne_features[:,1], 'label': treated_data['edibility']})\n",
    "\n",
    "sns.scatterplot(x = 'tsne_1', y = 'tsne_2', hue = 'label', data = tsne_df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data visualisation UMAP\n",
    "#UMAP works very similarly to tsne but get more efficient for higher volumes of data\n",
    "#result greatly affected by n_neighbors and min_dist hyperparameters\n",
    "#for n_neighbors try range 5-100 and min_dist 0.1-1\n",
    "fit = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    n_components=2,\n",
    "    metric = 'correlation'\n",
    ")\n",
    "\n",
    "umap_features = fit.fit_transform(data_scaled)\n",
    "\n",
    "umap_df = pd.DataFrame({'umap_1': umap_features[:,0], 'umap_2': umap_features[:,1], 'label': treated_data['edibility']})\n",
    "\n",
    "sns.scatterplot(x = 'umap_1', y = 'umap_2', hue = 'label', data = umap_df)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
