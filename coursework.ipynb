{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# - Notes for use to avoid confusion :)\n",
    "\n",
    "\n",
    "#Not all cells work together. For example if you run the cell that deals with missing values by removing the entire row\n",
    "#or removing all columns you will not be able to use the method to predict null values because the entries will be gone.\n",
    "\n",
    "#So in order to predict the missing values after using the code cells that remove them you simply have to first run the code cell\n",
    "#which reads the original data again\n",
    "\n",
    "#Websites:\n",
    "    #preporcessing methods outline - https://blog.ml.cmu.edu/2020/08/31/2-data-exploration/\n",
    "    #outliers simple - https://analyticsindiamag.com/how-to-detect-and-treat-outliers-in-categorical-data/\n",
    "    #pca collinearity removal/heatmap - https://towardsdatascience.com/how-do-you-apply-pca-to-logistic-regression-to-remove-multicollinearity-10b7f8e89f9b#:~:text=PCA%20in%20action%20to%20remove%20multicollinearity&text=PCA%20(Principal%20Component%20Analysis)%20takes,effectively%20eliminate%20multicollinearity%20between%20features.\n",
    "    #vif - https://towardsdatascience.com/how-to-remove-multicollinearity-using-python-4da8d9d8abb2\n",
    "    #pca 2D - https://towardsdatascience.com/principal-component-analysis-pca-from-scratch-in-python-7f3e2a540c51\n",
    "    #pca 3D - https://drzinph.com/pca-visualized-with-3d-scatter-plots/\n",
    "    #tsne -https://builtin.com/data-science/tsne-python\n",
    "    #umap - https://blog.ml.cmu.edu/2020/08/31/2-data-exploration/\n",
    "    #automatic outlier detection - https://machinelearningmastery.com/model-based-outlier-detection-and-removal-in-python/\n",
    "    #3D tsne and umap -  https://plotly.com/python/t-sne-and-umap-projections/#project-data-into-3d-with-tsne-and-pxscatter3d\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values have value ? all for attribute 11\n",
    "\n",
    "#either edible, poisonous or unknown. Unknowm and poisonous grouped together\n",
    "\n",
    "#to deal with missing values use 'classifier' or 'regressor' based on values that arent missing to predict missing values in\n",
    "#preprocessing. Be careful that vlaues using to predict arent overfitted. Compare to other methods such as removing entire attribute\n",
    "#or giving deafult/mean value\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import ensemble\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kai Heale\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# visualisation imports\n",
    "# separated to reduce import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import umap.umap_ as umap\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edibility                 2\n",
      "capShape                  6\n",
      "capSurface                4\n",
      "capColor                 10\n",
      "bruises                   2\n",
      "odor                      9\n",
      "gillAttachment            2\n",
      "gillSpacing               2\n",
      "gillSize                  2\n",
      "gillColor                12\n",
      "stalkShape                2\n",
      "stalkRoot                 4\n",
      "stalkSurfaceAboveRing     4\n",
      "stalkSurfaceBelowRing     4\n",
      "stalkColorAboveRing       9\n",
      "stalkColorBelowRing       9\n",
      "veilType                  1\n",
      "veilColor                 4\n",
      "ringNumber                3\n",
      "ringType                  5\n",
      "sporePrintColor           9\n",
      "population                6\n",
      "habitat                   7\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# read contents of data file\n",
    "file = pd.read_csv(\"agaricus-lepiota.data\", header = None)\n",
    "\n",
    "# create list to be used as headers\n",
    "features = ['edibility', 'capShape', 'capSurface', 'capColor', 'bruises', 'odor', \n",
    "            'gillAttachment', 'gillSpacing', 'gillSize', 'gillColor', \n",
    "            'stalkShape', 'stalkRoot', 'stalkSurfaceAboveRing', 'stalkSurfaceBelowRing', \n",
    "            'stalkColorAboveRing', 'stalkColorBelowRing', 'veilType', 'veilColor', \n",
    "            'ringNumber', 'ringType', 'sporePrintColor', 'population', 'habitat']\n",
    "\n",
    "#convert '?' to NaN\n",
    "file.replace({'?': np.nan}, inplace=True)\n",
    "\n",
    "# converting data frame to csv\n",
    "file.to_csv(\"agaricus-lepiota.csv\", header=features, index=False)\n",
    "\n",
    "data = pd.read_csv(\"agaricus-lepiota.csv\")\n",
    "\n",
    "#we can drop veilType as all entries are the same so cannot be used to predict\n",
    "print(data.nunique())\n",
    "data = data.drop('veilType', axis = 1).reset_index(drop=True)\n",
    "features.remove('veilType')\n",
    "\n",
    "#result = data.head(10)\n",
    "#print(data)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes column with missing values which in our data is only stalkRoot\n",
    "#one way to deal with missing values\n",
    "print(data.shape)\n",
    "\n",
    "data = data.drop('stalkRoot', axis = 1).reset_index(drop=True)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data.to_csv(\"agaricus-lepiota-no-stalkRoot.csv\")\n",
    "\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes all rows that have missing values\n",
    "print(data.shape)\n",
    "\n",
    "data = data.dropna(axis = 0).reset_index(drop=True)\n",
    " \n",
    "#shows we lose alot of data as 2480 rows lost\n",
    "#bad way of handling missing values\n",
    "print(data.shape)\n",
    "\n",
    "data.to_csv(\"agaricus-lepiota-NaN-removed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use label encoding on all features that are categorical\n",
    "#OneHot encoding not applicable as there are a high number of categories\n",
    "le = LabelEncoder()\n",
    "\n",
    "for i in range(len(features)):\n",
    "   data[i] = le.fit_transform(data[i])\n",
    "\n",
    "x = data['ringType'].value_counts()\n",
    "#print(x.index[4])\n",
    "\n",
    "#temp = data[data['capShape'] == 4]\n",
    "#print(temp['capShape'])\n",
    "\n",
    "data.to_csv(\"agaricus-lepiota-encoded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       724\n",
      "           1       1.00      1.00      1.00       119\n",
      "           2       1.00      1.00      1.00       239\n",
      "           3       1.00      1.00      1.00        47\n",
      "\n",
      "    accuracy                           1.00      1129\n",
      "   macro avg       1.00      1.00      1.00      1129\n",
      "weighted avg       1.00      1.00      1.00      1129\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       724\n",
      "           1       1.00      1.00      1.00       119\n",
      "           2       1.00      1.00      1.00       239\n",
      "           3       1.00      1.00      1.00        47\n",
      "\n",
      "    accuracy                           1.00      1129\n",
      "   macro avg       1.00      1.00      1.00      1129\n",
      "weighted avg       1.00      1.00      1.00      1129\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#predict missing values for stalkRoot\n",
    "NaN_predict = data.drop('edibility', axis = 1).reset_index(drop=True)\n",
    "\n",
    "NaN_train_data = NaN_predict[NaN_predict['stalkRoot']!= 4].copy()\n",
    "NaN_test_data = NaN_predict[NaN_predict['stalkRoot'] == 4].copy()\n",
    "\n",
    "NaN_X_train = NaN_train_data.drop('stalkRoot', axis = 1).reset_index(drop=True)\n",
    "NaN_y_train = NaN_train_data['stalkRoot']\n",
    "\n",
    "NaN_X_test = NaN_test_data.drop('stalkRoot', axis = 1).reset_index(drop=True)\n",
    "\n",
    "###############################################################################################################################\n",
    "#This code further breaks the data down into training and test sets comprising of rows that have no missing values\n",
    "#Allows us to run models and check how accurately they can predict the missing values then choose an appropriate one\n",
    "#In this case both Random Forest and Support Vector Classification predict that exact same values for 'stalkRoot' so either can be used\n",
    "#Both have 100% accuracy in predicitng 'stalkRoot' so we know when we predict the missing values they will most likely be correct.\n",
    "X_train, X_test, y_train, y_test = train_test_split(NaN_train_data, NaN_train_data['stalkRoot'], test_size=0.2,random_state=42)\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "svm = SVC()\n",
    "\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_pred = rfr.predict(X_test)\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "svm_pred = svm.predict(X_test)\n",
    "\n",
    "rfr_rounded = (np.rint(rfr_pred)).astype(int)\n",
    "\n",
    "svm_rounded = (np.rint(svm_pred)).astype(int)\n",
    "\n",
    "different = []\n",
    "\n",
    "for i in range(len(rfr_rounded)):\n",
    "    if rfr_rounded[i] != svm_rounded[i]:\n",
    "        different.append(rfr_rounded[i], svm_rounded[i])\n",
    "\n",
    "print(len(different))\n",
    "\n",
    "print(classification_report(y_test,rfr_rounded))\n",
    "\n",
    "print(classification_report(y_test,svm_rounded))\n",
    "##############################################################################################################################\n",
    "\n",
    "rfc = RandomForestRegressor()\n",
    "\n",
    "rfc.fit(NaN_X_train,NaN_y_train)\n",
    "y_pred = rfc.predict(NaN_X_test)\n",
    "\n",
    "rounded_data = (np.rint(y_pred)).astype(int)\n",
    "NaN_test_data['stalkRoot'] = rounded_data\n",
    "\n",
    "NaN_test_data.to_csv(\"predicted-missing-values.csv\")\n",
    "\n",
    "frames = [NaN_train_data, NaN_test_data]\n",
    "treated_data = pd.concat(frames)\n",
    "treated_data.insert(0, column = 'edibility', value = data['edibility'])\n",
    "\n",
    "X = treated_data.drop('edibility', axis = 1).reset_index(drop=True)\n",
    "y = treated_data['edibility']\n",
    "\n",
    "data_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "treated_data.to_csv(\"full-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8124, 22)\n",
      "(7619, 22)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m# now display histogram\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(contains_outliers)):\n\u001b[1;32m---> 26\u001b[0m     plt\u001b[39m.\u001b[39mhist(contains_outliers[x], alpha \u001b[39m=\u001b[39m \u001b[39m0.6\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# define outliers as values a category can take which is significantly more or less frequent than the others\n",
    "\n",
    "threshold = 0.01 # decides relative frequency threshold, below which values are considered outliers\n",
    "contains_outliers = []\n",
    "num_values = data.nunique()\n",
    "data = treated_data # uncomment if want to run on data after dealing with missing values\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "# loop through categories and identify which contain outliers according to the above definitions\n",
    "for i in range(len(features)):\n",
    "    if num_values[i] > 2:\n",
    "        value_frequencies = data[features[i]].value_counts(normalize=True) # normalize parameter converts values to relative frequency\n",
    "\n",
    "        for j in range(len(value_frequencies)):\n",
    "            if value_frequencies.iloc[j] < threshold:\n",
    "                data = data[data[features[i]] != value_frequencies.index[j]]\n",
    "                contains_outliers.append(value_frequencies)\n",
    "\n",
    "#print(contains_outliers)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "# now display histogram\n",
    "for x in range(len(contains_outliers)):\n",
    "    plt.hist(contains_outliers[x], alpha = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variance inflation factor\n",
    "\n",
    "fig = plt.figure(figsize=(25, 20))\n",
    "sns.heatmap(X.corr(), annot=True)\n",
    "plt.savefig(\"Heatmap.png\")\n",
    "\n",
    "#as you can see from heatmap there are some fearutes with high correlation\n",
    "\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X.columns\n",
    "\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
    "\t\t\t\t\t\tfor i in range(len(X.columns))]\n",
    "\n",
    "print(vif_data)\n",
    "\n",
    "features_to_drop = []\n",
    "\n",
    "for i in range(len(X.columns)):\n",
    "\tif vif_data[\"VIF\"][i] > 7:\n",
    "\t\tdrop = vif_data[\"VIF\"][i]\n",
    "\t\tfeatures_to_drop.append(vif_data[\"feature\"][i])\n",
    "\t\n",
    "uncorrelated_X = X.drop(features_to_drop, axis = 1).reset_index(drop=True)\n",
    "\n",
    "uncorrelated_X_scaled = StandardScaler().fit_transform(uncorrelated_X)\n",
    "\n",
    "#generally features with VIF > 5 get removed but we can experiment with slightly higher i.e > 7\n",
    "\n",
    "fig2 = plt.figure(figsize=(25, 20))\n",
    "sns.heatmap(uncorrelated_X.corr(), annot=True)\n",
    "plt.savefig(\"Heatmap2.png\")\n",
    "\n",
    "#as seen by second heatmap these have much less colinearality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data visualisation pca\n",
    "#a linear method so less complex than tsne thus takes much less time\n",
    "#not very useful to visualise our dataset I dont think as shown by the explained variance and scree plot\n",
    "pca = PCA(random_state=42)\n",
    "pca_features = pca.fit_transform(data_scaled)\n",
    "\n",
    "per_var = np.round(pca.explained_variance_ratio_*100, decimals = 1)\n",
    "print(per_var)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    "\n",
    "#to be seen on a 2D-plot we can only use PC1 and PC2 which gives us the highest explained variance but still very low\n",
    "#around 19% + 12% = 31% explained variance which is bad\n",
    "plt.bar(x= range(1,len(per_var)+1), height = per_var, tick_label = labels)\n",
    "plt.show()\n",
    "\n",
    "pca_df = pd.DataFrame({'pca_1': pca_features[:,0], 'pca_2': pca_features[:,1], 'label': treated_data['edibility']})\n",
    "print(pca_df)\n",
    "\n",
    "sns.scatterplot(x = 'pca_1', y = 'pca_2', hue = 'label', data = pca_df)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#PCA could be useful to reduce multicolinearality though instead of data visualisation\n",
    "#use the first 8 pc's to get about ~90% explained variance, 9 to get ~93%, 10 to get ~95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA 3D\n",
    "pca = PCA(n_components = 3, random_state=42)\n",
    "pca_features = pca.fit_transform(data_scaled)\n",
    "\n",
    "per_var = np.round(pca.explained_variance_ratio_*100, decimals = 1)\n",
    "print(per_var)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    "\n",
    "pca_df = pd.DataFrame({'pca_1': pca_features[:,0], 'pca_2': pca_features[:,1], 'pca_3': pca_features[:,2], 'label': treated_data['edibility']})\n",
    "\n",
    "edible = pca_df[pca_df['label'] == 0]\n",
    "poisonous = pca_df[pca_df['label'] == 1]\n",
    "\n",
    "colors=['b', 'r'] \n",
    "\n",
    "fig = plt.figure(1)\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "p1 = ax.plot(edible['pca_1'], \n",
    "             edible['pca_2'], \n",
    "             edible['pca_3'], \n",
    "             'o', color=colors[0],                                 \n",
    "             alpha = 0.6, label='edible',                           \n",
    "             markersize=3, \n",
    "             markeredgecolor='black',\n",
    "             markeredgewidth=0.1)\n",
    "\n",
    "p2 = ax.plot(poisonous['pca_1'], \n",
    "             poisonous['pca_2'], \n",
    "             poisonous['pca_3'], \n",
    "             'o', color=colors[1],                                 \n",
    "             alpha = 0.6, label='poisonous',                           \n",
    "             markersize=3, \n",
    "             markeredgecolor='black',\n",
    "             markeredgewidth=0.1)\n",
    "\n",
    "ax.set_xlabel('PCA-1, ' +  str(round(pca.explained_variance_ratio_[0]*100,2)) + '% Explained', fontsize=7)\n",
    "ax.set_ylabel('PCA-2, ' +  str(round(pca.explained_variance_ratio_[1]*100,2)) + '% Explained', fontsize=7)\n",
    "ax.set_zlabel('PCA-3, ' +  str(round(pca.explained_variance_ratio_[2]*100,2)) + '% Explained', fontsize=7)\n",
    "#z label wont work not sure why its 10.7% explained\n",
    "\n",
    "fig.legend(fontsize = 'x-small', loc='upper center', markerscale=2)\n",
    "plt.autoscale()\n",
    "plt.rcParams[\"figure.dpi\"] = 1000                            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data visualisation t-sne 2D\n",
    "#unlike pca its not linear so much more complex and better for our data I think\n",
    "#result of t-sne greatly affected by 'perplexity' hyperparameter\n",
    "#try perplexity values in the range 5-100 default is 30. Higher perplexity takes longer but may be better\n",
    "n_components = 2\n",
    "tsne = TSNE(n_components, perplexity=50)\n",
    "tsne_features = tsne.fit_transform(data_scaled)\n",
    "\n",
    "tsne_df = pd.DataFrame({'tsne_1': tsne_features[:,0], 'tsne_2': tsne_features[:,1], 'label': treated_data['edibility']})\n",
    "\n",
    "sns.scatterplot(x = 'tsne_1', y = 'tsne_2', hue = 'label', data = tsne_df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D tsne using plotly\n",
    "n_components = 3\n",
    "tsne = TSNE(n_components, perplexity=50, random_state=42)\n",
    "tsne_features = tsne.fit_transform(data_scaled)\n",
    "\n",
    "treated_data[\"edibility\"] = treated_data[\"edibility\"].astype(str)\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    tsne_features, x=0, y=1, z=2,\n",
    "    color=treated_data['edibility'], labels={'color': 'edibility'}\n",
    ")\n",
    "fig.update_traces(marker_size=8)\n",
    "fig.show()\n",
    "\n",
    "\"\"\"tsne = tsne = TSNE(n_components, perplexity=50, random_state = 42)\n",
    "tsne_features = tsne.fit_transform(uncorrelated_X_scaled)\n",
    "\n",
    "treated_data[\"edibility\"] = treated_data[\"edibility\"].astype(str)\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    tsne_features, x=0, y=1, z=2,\n",
    "    color=treated_data['edibility'], labels={'color': 'edibility'}\n",
    ")\n",
    "fig.update_traces(marker_size=8)\n",
    "fig.show()\"\"\"\n",
    "\n",
    "treated_data[\"edibility\"] = treated_data[\"edibility\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data visualisation UMAP\n",
    "#UMAP works very similarly to tsne but get more efficient for higher volumes of data\n",
    "#result greatly affected by n_neighbors and min_dist hyperparameters\n",
    "#for n_neighbors try range 5-100 and min_dist 0.1-1\n",
    "fit = umap.UMAP(\n",
    "    n_neighbors=50,\n",
    "    min_dist=0.25,\n",
    "    n_components=2,\n",
    "    metric = 'correlation',\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "umap_features = fit.fit_transform(data_scaled)\n",
    "\n",
    "umap_df = pd.DataFrame({'umap_1': umap_features[:,0], 'umap_2': umap_features[:,1], 'label': treated_data['edibility']})\n",
    "\n",
    "sns.scatterplot(x = 'umap_1', y = 'umap_2', hue = 'label', data = umap_df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UMAP 3D using plotly\n",
    "fit = umap.UMAP(\n",
    "    n_neighbors=50,\n",
    "    min_dist=0.25,\n",
    "    n_components=3,\n",
    "    metric = 'correlation',\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "umap_features = fit.fit_transform(data_scaled)\n",
    "\n",
    "treated_data[\"edibility\"] = treated_data[\"edibility\"].astype(str)\n",
    "\n",
    "fig_3d = px.scatter_3d(\n",
    "    umap_features, x=0, y=1, z=2,\n",
    "    color=treated_data['edibility'], labels={'color': 'edibility'}\n",
    ")\n",
    "fig_3d.update_traces(marker_size=5)\n",
    "\n",
    "fig_3d.show()\n",
    "\n",
    "\n",
    "\"\"\"fit = umap.UMAP(\n",
    "    n_neighbors=50,\n",
    "    min_dist=0.25,\n",
    "    n_components=3,\n",
    "    metric = 'correlation',\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "umap_features = fit.fit_transform(uncorrelated_X_scaled)\n",
    "\n",
    "treated_data[\"edibility\"] = treated_data[\"edibility\"].astype(str)\n",
    "\n",
    "fig_3d = px.scatter_3d(\n",
    "    umap_features, x=0, y=1, z=2,\n",
    "    color=treated_data['edibility'], labels={'color': 'edibility'}\n",
    ")\n",
    "fig_3d.update_traces(marker_size=5)\n",
    "\n",
    "fig_3d.show()\"\"\"\n",
    "\n",
    "treated_data[\"edibility\"] = treated_data[\"edibility\"].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Models__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7619, 22)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       821\n",
      "           1       1.00      1.00      1.00       804\n",
      "\n",
      "    accuracy                           1.00      1625\n",
      "   macro avg       1.00      1.00      1.00      1625\n",
      "weighted avg       1.00      1.00      1.00      1625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# knn\n",
    "# X, y stores baseline data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1) # hyperparameter: n_neighbors \n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "\n",
    "# knn_rounded = (np.rint(knn_pred)).astype(int)\n",
    "print(classification_report(y_test,knn_pred)) # evaluate model performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7438fcacdeb9fab963aae1492df335fc677df15640654dd4bab57d8a02162c47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
