{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Websites__<br>\n",
    "    preprocessing methods outline - https://blog.ml.cmu.edu/2020/08/31/2-data-exploration/<br>\n",
    "    outliers simple - https://analyticsindiamag.com/how-to-detect-and-treat-outliers-in-categorical-data/<br>\n",
    "    pca collinearity removal/heatmap - https://towardsdatascience.com/how-do-you-apply-pca-to-logistic-regression-to-remove-multicollinearity-10b7f8e89f9b#:~:text=PCA%20in%20action%20to%20remove%20multicollinearity&text=PCA%20(Principal%20Component%20Analysis)%20takes,effectively%20eliminate%20multicollinearity%20between%20features.<br>\n",
    "    vif - https://towardsdatascience.com/how-to-remove-multicollinearity-using-python-4da8d9d8abb2<br>\n",
    "    pca 2D - https://towardsdatascience.com/principal-component-analysis-pca-from-scratch-in-python-7f3e2a540c51<br>\n",
    "    pca 3D - https://drzinph.com/pca-visualized-with-3d-scatter-plots/<br>\n",
    "    tsne -https://builtin.com/data-science/tsne-python<br>\n",
    "    umap - https://blog.ml.cmu.edu/2020/08/31/2-data-exploration/<br>\n",
    "    automatic outlier detection - https://machinelearningmastery.com/model-based-outlier-detection-and-removal-in-python/<br>\n",
    "    3D t-sne and umap -  https://plotly.com/python/t-sne-and-umap-projections/#project-data-into-3d-with-tsne-and-pxscatter3d<br>\n",
    "    knn - https://medium.datadriveninvestor.com/k-nearest-neighbors-in-python-hyperparameters-tuning-716734bc557f<br>\n",
    "    hyperparameter tuning - https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74<br>\n",
    "    model evaluation - https://medium.com/@MohammedS/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b<br>\n",
    "    pydot and graphviz - https://github.com/WillKoehrsen/Data-Analysis/blob/master/random_forest_explained/Random%20Forest%20Explained.ipynb<br>\n",
    "\n",
    "    To do:\n",
    "    -Make diagrams look nice\n",
    "    -Compare metrics of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn imports\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import ensemble\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "#data handling imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation imports\n",
    "# separated to reduce import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import umap.umap_ as umap\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import pydot\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preprocessing and Data Exploration__<br>\n",
    "We want to predict edibility<br>\n",
    "It can take values edible, poisonous or unknown. Unknown and poisonous are grouped together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First read the file and format it as desired\n",
    "\n",
    "Missing values are stored as '?' characters so we convert to NaN to make the data easier to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell to read raw data\n",
    "\n",
    "#read contents of data file\n",
    "file = pd.read_csv(\"agaricus-lepiota.data\", header = None)\n",
    "\n",
    "# create list to be used as headers\n",
    "features = ['edibility', 'capShape', 'capSurface', 'capColor', 'bruises', 'odor', \n",
    "            'gillAttachment', 'gillSpacing', 'gillSize', 'gillColor', \n",
    "            'stalkShape', 'stalkRoot', 'stalkSurfaceAboveRing', 'stalkSurfaceBelowRing', \n",
    "            'stalkColorAboveRing', 'stalkColorBelowRing', 'veilType', 'veilColor', \n",
    "            'ringNumber', 'ringType', 'sporePrintColor', 'population', 'habitat']\n",
    "\n",
    "#convert '?' to NaN\n",
    "file.replace({'?': np.nan}, inplace=True)\n",
    "\n",
    "#converting data frame to csv\n",
    "file.to_csv(\"agaricus-lepiota.csv\", header=features, index=False)\n",
    "\n",
    "data = pd.read_csv(\"agaricus-lepiota.csv\")\n",
    "\n",
    "#result = data.head(10)\n",
    "#print(data)\n",
    "#print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple data exploration for illustration<br>\n",
    "Shows that the only feature containing missing values is 'stalkRoot', with 2480 null values\n",
    "\n",
    "It's impractical to perform univariate analysis on every single feature due to the high dimensionality, but we can at least illustrate some"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head(5)) # print first 5 rows\n",
    "print(data.shape) # number of rows and columns\n",
    "print(data.info()) # data types of each feature\n",
    "print(data.isnull().sum()) # show how many NaN values in each feature\n",
    "\n",
    "# show even split of values in the feature to be predicted\n",
    "data['edibility'] = LabelEncoder().fit_transform(data['edibility'])\n",
    "sns.countplot(x=data['edibility'])\n",
    "\n",
    "# this visualisation works best for features with a wide range of possible values\n",
    "# chose 3 features as a starting point\n",
    "\n",
    "f = plt.figure(figsize=(20,4))\n",
    "f.add_subplot(1,3,1)\n",
    "data['population'].value_counts().plot(kind='bar', color='red', xlabel='population', ylabel='count')\n",
    "f.add_subplot(1,3,2)\n",
    "data['ringNumber'].value_counts().plot(kind='bar', color='green', xlabel='ringNumber', ylabel='count')\n",
    "f.add_subplot(1,3,3)\n",
    "sns.countplot(x=data['gillColor'], color='yellow')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First need to use label encoding - OneHot encoding not applicable due to the high dimensionality of the dataset. OneHot Encoding would require using a large amount of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell for encoding the data\n",
    "\n",
    "#use label encoding on all features that are categorical\n",
    "le = LabelEncoder()\n",
    "\n",
    "for i in range(len(features)):\n",
    "   try:\n",
    "      data[features[i]] = le.fit_transform(data[features[i]])\n",
    "   except:\n",
    "      pass\n",
    "\n",
    "data.to_csv(\"agaricus-lepiota-encoded.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deletion methods of dealing with missing values<br>\n",
    "**Only use one of the 2 cell below at a time**\n",
    "\n",
    "**Don't use either if you're using the cell to predict missing values**\n",
    "\n",
    "1. Delete any column containing missing values\n",
    "2. Delete any row containing missing values\n",
    "\n",
    "Since only one feature contains missing values (stalkRoot) method 1 is most effective as it deletes less data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.\n",
    "#removes column with missing values which in our data is only stalkRoot\n",
    "#one way to deal with missing values\n",
    "print(data.shape)\n",
    "\n",
    "treated_data = data.drop('stalkRoot', axis = 1).reset_index(drop=True)\n",
    "\n",
    "features.remove(\"stalkRoot\")\n",
    "print(features)\n",
    "\n",
    "# show how much data is lost\n",
    "print(treated_data.shape)\n",
    "\n",
    "#convert dataframe to csv\n",
    "treated_data.to_csv(\"agaricus-lepiota-no-stalkRoot.csv\")\n",
    "\n",
    "#print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.\n",
    "#removes all rows that have missing values\n",
    "print(data.shape)\n",
    "\n",
    "treated_data = data[data['stalkRoot'] != 4].reset_index(drop=True)\n",
    " \n",
    "# show how much data is lost\n",
    "print(treated_data.shape)\n",
    "\n",
    "#convert dataframe to csv\n",
    "treated_data.to_csv(\"agaricus-lepiota-NaN-removed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better way to deal with missing values: Use classifier or regressor based on values which aren't missing to predict the missing values<br>\n",
    "Need to be careful to avoid overfitting (as with any predictor)\n",
    "\n",
    "Could also set the values to some default or mean value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict missing values for stalkRoot\n",
    "\n",
    "#drop 'edibility' from dataframe as it will be the target feature in the later models\n",
    "#It would be a bad idea to use 'edibility' to predict missing 'stalkRoot' values then use those values to later predict 'edibility'\n",
    "NaN_predict = data.drop('edibility', axis = 1).reset_index(drop=True)\n",
    "\n",
    "#split into set that contains missing values for 'stalkRoot' and set that does not contan missing values for 'stalkRoot'\n",
    "#The 'NaN' values are encoded to be '4' so they are removed\n",
    "NaN_train_data = NaN_predict[NaN_predict['stalkRoot']!= 4].copy()\n",
    "NaN_test_data = NaN_predict[NaN_predict['stalkRoot'] == 4].copy()\n",
    "\n",
    "#split data missing values into train and test set\n",
    "NaN_X_train = NaN_train_data.drop('stalkRoot', axis = 1).reset_index(drop=True)\n",
    "NaN_y_train = NaN_train_data['stalkRoot']\n",
    "\n",
    "NaN_X_test = NaN_test_data.drop('stalkRoot', axis = 1).reset_index(drop=True)\n",
    "\n",
    "###############################################################################################################################\n",
    "#This code further breaks the data down into training and test sets comprising of rows that have no missing values\n",
    "#Allows us to run models and check how accurately they can predict the missing values then choose an appropriate one\n",
    "#In this case both Random Forest and Support Vector Classification predict that exact same values for 'stalkRoot' so either can be used\n",
    "#Both have 100% accuracy in predicitng 'stalkRoot' so we know when we predict the missing values they will most likely be correct.\n",
    "\n",
    "#Split data with non missing values into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(NaN_train_data, NaN_train_data['stalkRoot'], test_size=0.2,random_state=42)\n",
    "\n",
    "#create classes for both classification models\n",
    "rfr = RandomForestClassifier()\n",
    "svm = SVC()\n",
    "\n",
    "#fit and predict both models\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr_pred = rfr.predict(X_test)\n",
    "\n",
    "svm.fit(X_train, y_train)\n",
    "svm_pred = svm.predict(X_test)\n",
    "\n",
    "rfr_rounded = (np.rint(rfr_pred)).astype(int)\n",
    "\n",
    "svm_rounded = (np.rint(svm_pred)).astype(int)\n",
    "\n",
    "#create a list and append all predicted values which arent the same\n",
    "different = []\n",
    "\n",
    "for i in range(len(rfr_rounded)):\n",
    "    if rfr_rounded[i] != svm_rounded[i]:\n",
    "        different.append(rfr_rounded[i], svm_rounded[i])\n",
    "\n",
    "#check how well models performed\n",
    "print(len(different))\n",
    "\n",
    "print(classification_report(y_test,rfr_rounded))\n",
    "\n",
    "print(classification_report(y_test,svm_rounded))\n",
    "##############################################################################################################################\n",
    "#now this code is to predict missing values\n",
    "\n",
    "#create random forest class\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "#fit and predict data\n",
    "rfc.fit(NaN_X_train,NaN_y_train)\n",
    "y_pred = rfc.predict(NaN_X_test)\n",
    "\n",
    "#set NaN values to be the predicted values\n",
    "rounded_data = (np.rint(y_pred)).astype(int)\n",
    "NaN_test_data['stalkRoot'] = rounded_data\n",
    "\n",
    "NaN_test_data.to_csv(\"predicted-missing-values.csv\")\n",
    "\n",
    "#combine the two dataframes to make the complete dataset\n",
    "frames = [NaN_train_data, NaN_test_data]\n",
    "treated_data = pd.concat(frames)\n",
    "treated_data = treated_data.sort_index()\n",
    "treated_data.insert(0, column = 'edibility', value = data['edibility'])\n",
    "\n",
    "treated_data.to_csv(\"full-data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create multiple datasets which can be preprocessed differently and compared later\n",
    "baseline_X = data.drop('edibility', axis = 1).reset_index(drop=True)\n",
    "baseline_y = data['edibility']\n",
    "\n",
    "X = treated_data.drop('edibility', axis = 1).reset_index(drop=True)\n",
    "y = treated_data['edibility'] \n",
    "\n",
    "#scale the data for correct visualisation\n",
    "data_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now deal with outliers<br>\n",
    "We define outliers as values a category can take which is significantly more or less frequent than the others<br>\n",
    "\n",
    "Detect outliers based on relative frequency and remove any rows containing outliers in at least one category<br>\n",
    "Then show histogram of data distribution within categories containing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell to detect and remove outliers\n",
    "\n",
    "#decides relative frequency threshold, below which values are considered outliers\n",
    "threshold = 0.01\n",
    "contains_outliers = []\n",
    "num_values = data.nunique()\n",
    "\n",
    "print(\"Original dataset:\",treated_data.shape)\n",
    "\n",
    "#loop through categories and identify which contain outliers according to the above definitions\n",
    "for i in range(len(features)):\n",
    "    if num_values[i] > 2:\n",
    "        value_frequencies = treated_data[features[i]].value_counts(normalize=True) #normalize parameter converts values to relative frequency\n",
    "\n",
    "        #loop through features that contain outliers and remove the all rows that contain the outlier values\n",
    "        for j in range(len(value_frequencies)):\n",
    "            if value_frequencies.iloc[j] < threshold:\n",
    "                treated_data = treated_data[treated_data[features[i]] != value_frequencies.index[j]]\n",
    "                contains_outliers.append(value_frequencies)\n",
    "\n",
    "#print(contains_outliers)\n",
    "\n",
    "print(\"Outliers removed:\",treated_data.shape)\n",
    "\n",
    "#now display histogram\n",
    "\n",
    "if len(contains_outliers) != 0:\n",
    "    for x in range(len(contains_outliers)):\n",
    "        outlier_histogram = plt.hist(contains_outliers[x], alpha = 0.6)\n",
    "\n",
    "    plt.xlabel(\"Relative frequency\")\n",
    "    plt.ylabel(\"Encoded label\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check for any features which only contain one value<br>\n",
    "These won't affect the model performance so we can remove them to simplify the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell to remove redundant features\n",
    "\n",
    "num_values = treated_data.nunique()\n",
    "#print(num_values)\n",
    "removed_features = []\n",
    "\n",
    "\n",
    "#treated_data = treated_data[treated_data[features[i]] != value_frequencies.index[j]]\n",
    "\n",
    "j = 0\n",
    "for i in range(len(features)):\n",
    "    if num_values[i] == 1:\n",
    "        treated_data = treated_data.drop(features[j],axis=1).reset_index(drop=True)\n",
    "        removed_features.append(features[j])\n",
    "        features.remove(features[j])\n",
    "    else:\n",
    "        j += 1\n",
    "\n",
    "X = treated_data.drop('edibility', axis = 1).reset_index(drop=True) #update treated X\n",
    "y = treated_data['edibility'] #update treated y\n",
    "\n",
    "print(\"Removed\",removed_features)\n",
    "#print(treated_data.nunique())\n",
    "#data = data.drop('veilType', axis = 1).reset_index(drop=True)\n",
    "#features.remove('veilType')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display heatmap to explore correlation between features<br>\n",
    "Multicollinearity can lead to overfitting so need to remove correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell that uses variance inflation factor to identify and remove highly correlated data\n",
    "\n",
    "#create a heatmap showing feature correlation\n",
    "fig = plt.figure(figsize=(25, 20))\n",
    "sns.heatmap(X.corr(), annot=True)\n",
    "plt.savefig(\"Heatmap.png\")\n",
    "\n",
    "#as you can see from heatmap there are some features with high correlation\n",
    "\n",
    "#create dataframe with all features and their VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X.columns\n",
    "\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
    "\t\t\t\t\t\tfor i in range(len(X.columns))]\n",
    "\n",
    "print(vif_data)\n",
    "\n",
    "features_to_drop = []\n",
    "\n",
    "#loop through the features and add those with VIF higher than 7 to a list\n",
    "for i in range(len(X.columns)):\n",
    "\tif vif_data[\"VIF\"][i] > 5:\n",
    "\t\tdrop = vif_data[\"VIF\"][i]\n",
    "\t\tfeatures_to_drop.append(vif_data[\"feature\"][i])\n",
    "\n",
    "#create new dataframe identified features removed\n",
    "uncorrelated_X = X.drop(features_to_drop, axis = 1).reset_index(drop=True)\n",
    "\n",
    "#scale for visualisation\n",
    "uncorrelated_X_scaled = StandardScaler().fit_transform(uncorrelated_X)\n",
    "\n",
    "cleaned_features = uncorrelated_X.columns\n",
    "\n",
    "#generally features with VIF > 5 get removed but we can experiment with slightly higher i.e > 7\n",
    "\n",
    "#create heatmap to compare feature correlation of new dataframe\n",
    "fig2 = plt.figure(figsize=(25, 20))\n",
    "sns.heatmap(uncorrelated_X.corr(), annot=True)\n",
    "plt.savefig(\"Heatmap2.png\")\n",
    "\n",
    "#as seen by second heatmap these have much less colinearality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Data Visualisation__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. PCA<br>\n",
    "Linear method so less complex than T-SNE, meaning it takes much less time<br>\n",
    "But not so useful to visualise our dataset as shown by the explained variance and scree plot<br>\n",
    "\n",
    "However it could be useful to reduce multicollinearity instead of data visualisation<br>\n",
    "Could use the first 8 pc's to get about ~90% explained variance, 9 to get ~93%, 10 to get ~95% which greatly reduces the dimesionality of out dataset while mostly maintaining important information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data visualisation pca 2D\n",
    "pca = PCA(random_state=42)\n",
    "pca_features = pca.fit_transform(data_scaled)\n",
    "\n",
    "#calculate percieved variance\n",
    "per_var = np.round(pca.explained_variance_ratio_*100, decimals = 1)\n",
    "print(per_var)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,8)\n",
    "\n",
    "#to be seen on a 2D-plot we can only use PC1 and PC2 which gives us the highest explained variance but still very low\n",
    "#around 19% + 12% = 31% explained variance which is bad\n",
    "plt.bar(x= range(1,len(per_var)+1), height = per_var, tick_label = labels)\n",
    "plt.xlabel(\"PCA NO.\")\n",
    "plt.ylabel(\"Explained Variance\")\n",
    "plt.show()\n",
    "plt.savefig(\"PCA_distribution\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]\n",
    "\n",
    "pca_df = pd.DataFrame({'pca_1': pca_features[:,0], 'pca_2': pca_features[:,1], 'Edibility': treated_data['edibility']})\n",
    "print(pca_df)\n",
    "\n",
    "#plot pca 1 and pca 2\n",
    "sns.scatterplot(x = 'pca_1', y = 'pca_2', hue = 'Edibility', data = pca_df)\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(\"2D PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA 3D\n",
    "pca = PCA(n_components = 3, random_state=42)\n",
    "pca_features = pca.fit_transform(data_scaled)\n",
    "\n",
    "#calculate percieved variance\n",
    "per_var = np.round(pca.explained_variance_ratio_*100, decimals = 1)\n",
    "print(per_var)\n",
    "labels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n",
    "\n",
    "pca_df = pd.DataFrame({'pca_1': pca_features[:,0], 'pca_2': pca_features[:,1], 'pca_3': pca_features[:,2], 'label': treated_data['edibility']})\n",
    "\n",
    "print(pca_features)\n",
    "\n",
    "#plot pca 3D\n",
    "edible = pca_df[pca_df['label'] == 0]\n",
    "poisonous = pca_df[pca_df['label'] == 1]\n",
    "\n",
    "colors=['b', 'r'] \n",
    "\n",
    "fig = plt.figure(1)\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "p1 = ax.plot(edible['pca_1'], \n",
    "             edible['pca_2'], \n",
    "             edible['pca_3'], \n",
    "             'o', color=colors[0],                                 \n",
    "             alpha = 0.6, label='edible',                           \n",
    "             markersize=3, \n",
    "             markeredgecolor='black',\n",
    "             markeredgewidth=0.1)\n",
    "\n",
    "p2 = ax.plot(poisonous['pca_1'], \n",
    "             poisonous['pca_2'], \n",
    "             poisonous['pca_3'], \n",
    "             'o', color=colors[1],                                 \n",
    "             alpha = 0.6, label='poisonous',                           \n",
    "             markersize=3, \n",
    "             markeredgecolor='black',\n",
    "             markeredgewidth=0.1)\n",
    "\n",
    "ax.set_xlabel('PCA-1, ' +  str(round(pca.explained_variance_ratio_[0]*100,2)) + '% Explained', fontsize=7)\n",
    "ax.set_ylabel('PCA-2, ' +  str(round(pca.explained_variance_ratio_[1]*100,2)) + '% Explained', fontsize=7)\n",
    "ax.set_zlabel('PCA-3, ' +  str(round(pca.explained_variance_ratio_[2]*100,2)) + '% Explained', fontsize=7)\n",
    "#z label wont work not sure why. It should be '10.7% Explained'\n",
    "\n",
    "fig.legend(fontsize = 'x-small', loc='upper center', markerscale=2)\n",
    "plt.autoscale()\n",
    "plt.rcParams[\"figure.dpi\"] = 1000                            \n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. T-SNE<br>\n",
    "Unlike pca, it's not linear so much more complex and better for our data<br>\n",
    "The result of t-sne is greatly affected by the 'perplexity' hyperparameter<br>\n",
    "Try perplexity values in the range 5-100 default is 30. Higher perplexity takes longer but may be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data visualisation t-sne 2D\n",
    "n_components = 2\n",
    "tsne = TSNE(n_components, perplexity=50)\n",
    "tsne_features = tsne.fit_transform(data_scaled)\n",
    "\n",
    "tsne_df = pd.DataFrame({'tsne_1': tsne_features[:,0], 'tsne_2': tsne_features[:,1], 'label': treated_data['edibility']})\n",
    "\n",
    "#plot tnse 1 and tsne 2\n",
    "sns.scatterplot(x = 'tsne_1', y = 'tsne_2', hue = 'label', data = tsne_df)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D t-SNE using plotly\n",
    "n_components = 3\n",
    "\n",
    "#create t-SNE class and set hyperparameters\n",
    "tsne = TSNE(n_components, perplexity=50, random_state=42)\n",
    "tsne_features = tsne.fit_transform(data_scaled)\n",
    "\n",
    "treated_data[\"edibility\"] = treated_data[\"edibility\"].astype(str)\n",
    "\n",
    "#plot t-SNE\n",
    "fig = px.scatter_3d(\n",
    "    tsne_features, x=0, y=1, z=2,\n",
    "    color=treated_data['edibility'], labels={'color': 'edibility'},\n",
    "    title = \"Dimensionality reduction using t-SNE to visualise the dataset\"\n",
    ")\n",
    "fig.update_traces(marker_size=8)\n",
    "\n",
    "fig.update_layout(scene = dict(\n",
    "                    xaxis_title='t-SNE 1',\n",
    "                    yaxis_title='t-SNE 2',\n",
    "                    zaxis_title='t-SNE 3')\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig.write_html(\"/Users/will/Documents/Uni/Year 2/Artificial Intelligence/Machine Learning/ML-Coursework-2022/t-SNE-3D.html\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. UMAP <br>\n",
    "UMAP works very similarly to tsne but gets more efficient for higher volumes of data <br>\n",
    "Result greatly affected by n_neighbors and min_dist hyperparameters <br>\n",
    "For n_neighbors try range 5-100 and min_dist 0.1-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data visualisation UMAP\n",
    "\n",
    "#create UMAP class and set hyperparameters\n",
    "fit = umap.UMAP(\n",
    "    n_neighbors=50,\n",
    "    min_dist=0.25,\n",
    "    n_components=2,\n",
    "    metric = 'correlation',\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "umap_features = fit.fit_transform(data_scaled)\n",
    "\n",
    "umap_df = pd.DataFrame({'umap_1': umap_features[:,0], 'umap_2': umap_features[:,1], 'label': treated_data['edibility']})\n",
    "\n",
    "\n",
    "#plot 2D UMAP\n",
    "sns.scatterplot(x = 'umap_1', y = 'umap_2', hue = 'label', data = umap_df)\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UMAP 3D using plotly\n",
    "\n",
    "#create UMAP class and set hyperparameters\n",
    "fit = umap.UMAP(\n",
    "    n_neighbors=75,\n",
    "    min_dist=0.5,\n",
    "    n_components=3,\n",
    "    metric = 'correlation',\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "umap_features = fit.fit_transform(data_scaled)\n",
    "\n",
    "treated_data[\"edibility\"] = treated_data[\"edibility\"].astype(str)\n",
    "\n",
    "#plot UMAP 3D\n",
    "fig_3d = px.scatter_3d(\n",
    "    umap_features, x=0, y=1, z=2,\n",
    "    color=treated_data['edibility'], labels={'color': 'edibility'},\n",
    "    title = \"Dimensionality reduction using UMAP to visualise the dataset\"\n",
    ")\n",
    "fig_3d.update_traces(marker_size=5)\n",
    "\n",
    "fig.update_layout(scene = dict(\n",
    "                    xaxis_title='UMAP 1',\n",
    "                    yaxis_title='UMAP 2',\n",
    "                    zaxis_title='UMAP 3')\n",
    ")\n",
    "\n",
    "fig_3d.show()\n",
    "\n",
    "fig_3d.write_html(\"/Users/will/Documents/Uni/Year 2/Artificial Intelligence/Machine Learning/ML-Coursework-2022/UMAP-3D.html\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Models__ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare baseline data (baseline_X and baseline_y) with treated data (uncorrelated_X and y)\n",
    "\n",
    "1. K-nearest neighbours<br>\n",
    "Hyperparameter: n_neighbors<br>\n",
    "Low value less accurate, high value overfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell for  knn model\n",
    "\n",
    "# hyperparameters\n",
    "n_neighbors = list(range(1,30))\n",
    "weights = ['uniform','distance']\n",
    "algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "leaf_size = list(range(1,50))\n",
    "p = [1,2]\n",
    "\n",
    "# create dictionary\n",
    "random_grid = {'n_neighbors': n_neighbors,\n",
    "               'weights': weights,\n",
    "               'algorithm': algorithm,\n",
    "               'leaf_size': leaf_size,\n",
    "               'p': p}\n",
    "\n",
    "# baseline model\n",
    "knn_baseline = KNeighborsClassifier()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(uncorrelated_X, y, test_size=0.2)\n",
    "knn_baseline.fit(X_train, y_train)\n",
    "baseline_knn_pred = knn_baseline.predict(X_test)\n",
    "\n",
    "# random hyperparameter values\n",
    "knn_random = RandomizedSearchCV(estimator = knn_baseline, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1, scoring = 'accuracy')\n",
    "\n",
    "knn_random.fit(X_train, y_train)\n",
    "\n",
    "best_random_values = knn_random.best_params_\n",
    "print(\"\\n\", best_random_values, \"\\n\")\n",
    "best_random_knn = knn_random.best_estimator_\n",
    "print(best_random_knn)\n",
    "\n",
    "#predict using best random model and get model accuracy report\n",
    "best_random_pred = best_random_knn.predict(X_test)\n",
    "\n",
    "# best: {'weights': 'uniform', 'p': 2, 'n_neighbors': 14, 'leaf_size': 47, 'algorithm': 'auto'} \n",
    "\n",
    "#create a refined hyperparameter grid with values close to the best random estimator to look for further optimization\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range((best_random_values['n_neighbors']-20),(best_random_values['n_neighbors']+20))),\n",
    "    'weights': [best_random_values['weights']],\n",
    "    'algorithm': [best_random_values['algorithm']],\n",
    "    'leaf_size': list(range((best_random_values['leaf_size']-20),(best_random_values['leaf_size']+20))),\n",
    "    'p': [best_random_values['p']]\n",
    "}\n",
    "\n",
    "knn_refined = KNeighborsClassifier()\n",
    "\n",
    "# find the best scoring model using the refined hyperparameter grid\n",
    "knn_grid = GridSearchCV(estimator = knn_refined, param_grid = param_grid, cv = 3, verbose=2, n_jobs = -1, scoring = 'accuracy')\n",
    "\n",
    "knn_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\", knn_grid.best_params_, \"\\n\")\n",
    "best_grid = knn_grid.best_estimator_\n",
    "print(best_grid)\n",
    "\n",
    "# predict with refined model\n",
    "best_refined_pred = best_grid.predict(X_test)\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2)\n",
    "\n",
    "#show model accuracy report\n",
    "print(\"Baseline\\n\")\n",
    "print(classification_report(y_test,baseline_knn_pred))\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, baseline_knn_pred)\n",
    "\n",
    "print(cf_matrix)\n",
    "\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), cmap = 'Blues', annot = True, ax = ax1)\n",
    "\n",
    "print(\"Random Search\\n\")\n",
    "print(classification_report(y_test,best_random_pred))\n",
    "\n",
    "cf_matrix_2 = confusion_matrix(y_test, best_random_pred)\n",
    "\n",
    "print(cf_matrix_2)\n",
    "\n",
    "#sns.heatmap(cf_matrix_2/np.sum(cf_matrix_2), cmap = 'Blues', annot = True, ax = ax2)\n",
    "\n",
    "print(\"Grid Search\\n\")\n",
    "print(classification_report(y_test,best_refined_pred))\n",
    "\n",
    "cf_matrix_3 = confusion_matrix(y_test, best_refined_pred)\n",
    "\n",
    "print(cf_matrix_3)\n",
    "\n",
    "sns.heatmap(cf_matrix_3/np.sum(cf_matrix_3), cmap = 'Blues', annot = True, ax = ax2)\n",
    "\n",
    "'''\n",
    "#get one tree from estimators\n",
    "tree = best_grid.estimators_[5]\n",
    "\n",
    "#Export the image to a dot file then use file to creat a graph\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = cleaned_features, rounded = True, precision = 1)\n",
    "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('knn_tree.png')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell for RandomForestClassifier\n",
    "\n",
    "#create distribution of chosen hyperparamaters to be used in random search\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [3, 5, 10]\n",
    "min_samples_leaf = [2, 4, 6]\n",
    "bootstrap = [True, False]\n",
    "\n",
    "labels = ('True Negative', 'False Positive', 'Fale Negative', 'True Positive')\n",
    "\n",
    "#create dictionary with distribution of chosen hyperparameters\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "#basline model to compare to when optimising\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(uncorrelated_X, y, test_size=0.2)\n",
    "\n",
    "baseline_rfc = RandomForestClassifier(random_state = 42, n_estimators = 10)\n",
    "\n",
    "baseline_rfc.fit(X_train, y_train)\n",
    "baseline_rfc_pred = baseline_rfc.predict(X_test)\n",
    "\n",
    "\n",
    "#find best scoring random hyperparameter combination\n",
    "rfc_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1, scoring = 'accuracy')\n",
    "\n",
    "rfc_random.fit(X_train, y_train)\n",
    "best_random_values = rfc_random.best_params_\n",
    "\n",
    "print(\"\\n\", rfc_random.best_params_, \"\\n\")\n",
    "best_random_rfc = rfc_random.best_estimator_\n",
    "print(best_random_rfc)\n",
    "\n",
    "#predict using best random model and get model accuracy report\n",
    "best_pred = best_random_rfc.predict(X_test)\n",
    "\n",
    "#create a refined hyperparameter grid with values close to the best random estimator to look for further optimization\n",
    "param_grid = {\n",
    "    'n_estimators': list(range((best_random_values['n_estimators']-20),(best_random_values['n_estimators']+20))),\n",
    "    'max_features': [best_random_values['max_features']],\n",
    "    'max_depth': list(range((best_random_values['max_depth']-9),(best_random_values['max_depth']+10))),\n",
    "    'min_samples_split': list(range((best_random_values['min_samples_split']-1),(best_random_values['min_samples_split']+1))),\n",
    "    'min_samples_split': list(range((best_random_values['min_samples_leaf']-1),(best_random_values['min_samples_leaf']+1))),\n",
    "    'bootstrap': [best_random_values['bootstrap']]\n",
    "}\n",
    "\n",
    "rf2 = RandomForestClassifier()\n",
    "\n",
    "#find the best scoring model using the refined hyperparameter grid\n",
    "rfc_grid = GridSearchCV(estimator = rf2, param_grid = param_grid, cv = 3, verbose=2, n_jobs = -1, scoring = 'accuracy')\n",
    "\n",
    "rfc_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\", rfc_grid.best_params_, \"\\n\")\n",
    "best_grid = rfc_grid.best_estimator_\n",
    "print(best_grid)\n",
    "\n",
    "#predict values using this model and show accuracy to compare models later\n",
    "best_pred2 = best_grid.predict(X_test)\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2)\n",
    "\n",
    "#show model accuracy report\n",
    "print(\"Baseline\\n\")\n",
    "print(classification_report(y_test,baseline_rfc_pred))\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, baseline_rfc_pred)\n",
    "\n",
    "print(cf_matrix)\n",
    "\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), cmap = 'Blues', annot = True, ax = ax1)\n",
    "\n",
    "print(\"Random Search\\n\")\n",
    "print(classification_report(y_test,best_pred))\n",
    "\n",
    "cf_matrix_2 = confusion_matrix(y_test, best_pred)\n",
    "\n",
    "print(cf_matrix_2)\n",
    "\n",
    "#sns.heatmap(cf_matrix_2/np.sum(cf_matrix_2), cmap = 'Blues', annot = True, ax = ax2)\n",
    "\n",
    "print(\"Grid Search\\n\")\n",
    "print(classification_report(y_test,best_pred2))\n",
    "\n",
    "cf_matrix_3 = confusion_matrix(y_test, best_pred2)\n",
    "\n",
    "print(cf_matrix_3)\n",
    "\n",
    "sns.heatmap(cf_matrix_3/np.sum(cf_matrix_3), cmap = 'Blues', annot = True, ax = ax2)\n",
    "\n",
    "#get one tree from estimators\n",
    "tree = best_grid.estimators_[5]\n",
    "\n",
    "#Export the image to a dot file then use file to creat a graph\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = cleaned_features, rounded = True, precision = 1)\n",
    "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell for SVM\n",
    "\n",
    "#create distribution of chosen hyperparamaters to be used in SVM\n",
    "gamma = [20,50,100,200,400]\n",
    "kernel = ['linear', 'rbf']\n",
    "c = [20,50,100,200,400]\n",
    "degree = [1,2,3,4,5,6]\n",
    "\n",
    "#create dictionary with distribution of chosen hyperparameters\n",
    "random_grid = {'gamma': gamma,\n",
    "               'kernel': kernel,\n",
    "               'C': c,\n",
    "               'degree': degree}\n",
    "\n",
    "print(random_grid)\n",
    "\n",
    "#basline model to compare to when optimising\n",
    "svc = SVC(random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(uncorrelated_X, y, test_size=0.2)\n",
    "\n",
    "baseline_SVC = SVC(random_state = 42)\n",
    "\n",
    "baseline_SVC.fit(X_train, y_train)\n",
    "baseline_SVC_pred = baseline_SVC.predict(X_test)\n",
    "\n",
    "#find best scoring random hyperparameter combination\n",
    "SVC_random = RandomizedSearchCV(estimator = svc, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1, scoring = 'accuracy')\n",
    "\n",
    "SVC_random.fit(X_train, y_train)\n",
    "best_random_values = SVC_random.best_params_\n",
    "\n",
    "print(\"\\n\", SVC_random.best_params_, \"\\n\")\n",
    "best_random_SVC = SVC_random.best_estimator_\n",
    "print(best_random_SVC)\n",
    "\n",
    "#predict using best random model and get model accuracy report\n",
    "best_pred = best_random_SVC.predict(X_test)\n",
    "\n",
    "#create a refined hyperparameter grid with values close to the best random estimator to look for further optimization\n",
    "\n",
    "param_grid = {\n",
    "    'gamma': list(range((best_random_values['gamma']-10),(best_random_values['gamma']+10))),\n",
    "    'kernel': [best_random_values['kernel']],\n",
    "    'C': list(range((best_random_values['C']-10),(best_random_values['C']+10))),\n",
    "    'degree': [best_random_values['degree']],\n",
    "}\n",
    "\n",
    "svc2 = SVC(random_state = 42)\n",
    "\n",
    "#find the best scoring model using the refined hyperparameter grid\n",
    "svc_grid = GridSearchCV(estimator = svc2, param_grid = param_grid, cv = 3, verbose=2, n_jobs = -1, scoring = 'accuracy')\n",
    "\n",
    "svc_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n\", svc_grid.best_params_, \"\\n\")\n",
    "best_grid = svc_grid.best_estimator_\n",
    "print(best_grid)\n",
    "\n",
    "#predict values using this model and show accuracy to compare models later\n",
    "best_pred2 = best_grid.predict(X_test)\n",
    "\n",
    "print(\"\\n\", svc_grid.best_params_, \"\\n\")\n",
    "best_grid = svc_grid.best_estimator_\n",
    "print(best_grid)\n",
    "\n",
    "#predict values using this model and show accuracy to compare models later\n",
    "best_pred2 = best_grid.predict(X_test)\n",
    "\n",
    "fig,(ax1,ax2) = plt.subplots(1,2)\n",
    "\n",
    "#show model accuracy report\n",
    "print(\"Baseline\\n\")\n",
    "print(classification_report(y_test,baseline_SVC_pred))\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, baseline_SVC_pred)\n",
    "\n",
    "print(cf_matrix)\n",
    "\n",
    "#sns.heatmap(cf_matrix/np.sum(cf_matrix), cmap = 'Blues', annot = True, ax = ax1)\n",
    "\n",
    "print(\"Random Search\\n\")\n",
    "print(classification_report(y_test,best_pred))\n",
    "\n",
    "cf_matrix_2 = confusion_matrix(y_test, best_pred)\n",
    "\n",
    "print(cf_matrix_2)\n",
    "\n",
    "#sns.heatmap(cf_matrix_2/np.sum(cf_matrix_2), cmap = 'Blues', annot = True, ax = ax2)\n",
    "\n",
    "print(\"Grid Search\\n\")\n",
    "print(classification_report(y_test,best_pred2))\n",
    "\n",
    "cf_matrix_3 = confusion_matrix(y_test, best_pred2)\n",
    "\n",
    "print(cf_matrix_3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
